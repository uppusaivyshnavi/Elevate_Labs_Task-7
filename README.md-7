 1. Importing Required Libraries
python
Copy
Edit
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
numpy: For numerical operations.

matplotlib.pyplot: For plotting graphs and visualizations.

seaborn: High-level plotting (used for aesthetic enhancements).

sklearn.datasets.load_breast_cancer: Loads a binary classification dataset.

train_test_split: Splits data into training and test sets.

GridSearchCV: Used to tune hyperparameters (like C and gamma).

cross_val_score: Performs cross-validation to evaluate model performance.

StandardScaler: Standardizes features (mean = 0, std = 1).

SVC: Support Vector Classification model.

classification_report, confusion_matrix, accuracy_score: Metrics to evaluate model performance.

PCA: Principal Component Analysis for dimensionality reduction.

 2. Load and Prepare Dataset
python
Copy
Edit
data = load_breast_cancer()
X = data.data
y = data.target
Loads the breast cancer dataset.

X: Feature matrix (30 features).

y: Target values (0 = malignant, 1 = benign).

 3. Standardize the Dataset
python
Copy
Edit
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
Standardization is essential for SVM to perform optimally.

All features are rescaled to have mean = 0 and standard deviation = 1.

 4. Train-Test Split
python
Copy
Edit
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
Splits the dataset into 70% training and 30% testing.

Ensures reproducibility with random_state.

 5. Train Linear SVM
python
Copy
Edit
linear_svm = SVC(kernel='linear', C=1.0)
linear_svm.fit(X_train, y_train)
y_pred_linear = linear_svm.predict(X_test)
Uses a linear kernel (good for linearly separable data).

Trains on the training data and predicts on test data.

 6. Evaluate Linear SVM
python
Copy
Edit
print("Accuracy:", accuracy_score(y_test, y_pred_linear))
print(confusion_matrix(y_test, y_pred_linear))
print(classification_report(y_test, y_pred_linear))
Calculates and prints:

Accuracy

Confusion matrix (TP, FP, TN, FN)

Precision, recall, F1-score

 7. Train RBF Kernel SVM
python
Copy
Edit
rbf_svm = SVC(kernel='rbf', C=1.0, gamma='scale')
rbf_svm.fit(X_train, y_train)
y_pred_rbf = rbf_svm.predict(X_test)
RBF (Radial Basis Function) kernel handles non-linear relationships.

gamma='scale' adjusts how far the influence of a single training example reaches.

 8. Evaluate RBF SVM
Similar to the linear model:

python
Copy
Edit
print("Accuracy:", accuracy_score(y_test, y_pred_rbf))
 9. Hyperparameter Tuning with GridSearchCV
python
Copy
Edit
param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.01, 0.001], 'kernel': ['rbf']}
grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5)
grid.fit(X_train, y_train)
Tries multiple combinations of C and gamma to find the best values.

Performs 5-fold cross-validation.

refit=True: Once best params are found, it refits the model.

 10. Use Best Model
python
Copy
Edit
grid_predictions = grid.predict(X_test)
print("Accuracy (Tuned):", accuracy_score(y_test, grid_predictions))
Tests the model obtained after hyperparameter tuning.

 11. Cross-validation Accuracy
python
Copy
Edit
cv_scores = cross_val_score(grid.best_estimator_, X_scaled, y, cv=5)
print("Cross-validation accuracy: ", np.mean(cv_scores))
Uses the best model found by GridSearchCV.

Performs 5-fold cross-validation on the entire dataset for reliability.

 12. Visualization with PCA (2D Decision Boundary)
python
Copy
Edit
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_scaled)
Reduces the dataset from 30 dimensions to 2D using PCA.

Useful for visualizing the decision boundary.

python
Copy
Edit
svm_vis = SVC(kernel='rbf', C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])
svm_vis.fit(X_reduced, y)
Trains SVM again but on PCA-reduced features.

 13. Plotting Decision Boundary
python
Copy
Edit
# Generate meshgrid
xx, yy = np.meshgrid(np.linspace(...), np.linspace(...))
Z = svm_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plotting
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='coolwarm', edgecolors='k')
Creates a colored decision region (background) using contourf.

Plots actual data points over it.

Gives a clear visual representation of the modelâ€™s classification.

